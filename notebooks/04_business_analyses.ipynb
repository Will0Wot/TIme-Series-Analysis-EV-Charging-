{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "716699e5",
   "metadata": {},
   "source": [
    "\n",
    "# Business Analyses: Reliability, Load, and Impact\n",
    "\n",
    "Statistical playbook for the EV reliability dataset. Set `DATABASE_URL` in your env (Streamlit Cloud secret) to query the hosted DB (Neon/Timescale). Each section is runnable on its own; queries are scoped to recent windows to stay lightweight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import statsmodels.api as sm\n",
    "\n",
    "DATABASE_URL = os.getenv(\n",
    "    \"DATABASE_URL\",\n",
    "    \"postgresql://neondb_owner:npg_98OQEAezCaKH@ep-round-moon-ah39km2x-pooler.c-3.us-east-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require\",\n",
    ")\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "def fetch_df(sql, params=None):\n",
    "    return pd.read_sql(text(sql), engine, params=params or {})\n",
    "\n",
    "FAULT_STATES = (\"FAULTED\", \"OFFLINE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90c71b",
   "metadata": {},
   "source": [
    "\n",
    "## Reliability uplift (before/after or A/B)\n",
    "Compare fault rates across two windows and compute confidence intervals and a two-proportion z-test. Swap `window_before`/`window_after` to represent pre/post maintenance or A/B cohorts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06535e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import timedelta, datetime, timezone\n",
    "\n",
    "def window_counts(days: int, end=None):\n",
    "    end = end or datetime.now(timezone.utc)\n",
    "    start = end - timedelta(days=days)\n",
    "    sql = \"\"\"\n",
    "    SELECT\n",
    "        SUM(CASE WHEN status IN :faults THEN 1 ELSE 0 END) AS faults,\n",
    "        COUNT(*) AS samples\n",
    "    FROM charger_status\n",
    "    WHERE time BETWEEN :start AND :end\n",
    "    \"\"\"\n",
    "    row = fetch_df(sql, {\"faults\": FAULT_STATES, \"start\": start, \"end\": end}).iloc[0]\n",
    "    return row.faults, row.samples\n",
    "\n",
    "def compare_windows(window_before=7, window_after=7):\n",
    "    now = datetime.now(timezone.utc)\n",
    "    faults_after, n_after = window_counts(window_after, end=now)\n",
    "    faults_before, n_before = window_counts(window_before, end=now - timedelta(seconds=1))\n",
    "    rates = {\n",
    "        \"before\": faults_before / n_before if n_before else np.nan,\n",
    "        \"after\": faults_after / n_after if n_after else np.nan,\n",
    "    }\n",
    "    stat, p = proportions_ztest([faults_after, faults_before], [n_after, n_before])\n",
    "    return {\n",
    "        \"counts\": {\n",
    "            \"before_faults\": faults_before,\n",
    "            \"before_samples\": n_before,\n",
    "            \"after_faults\": faults_after,\n",
    "            \"after_samples\": n_after,\n",
    "        },\n",
    "        \"rates\": rates,\n",
    "        \"z_stat\": float(stat),\n",
    "        \"p_value\": float(p),\n",
    "    }\n",
    "\n",
    "compare_windows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cbd2b3",
   "metadata": {},
   "source": [
    "\n",
    "## Load vs faults (regression)\n",
    "Model whether higher utilization increases fault odds. We aggregate hourly by charger, join session counts, and fit a logistic regression on a fault flag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hourly aggregation by charger (status + session load)\n",
    "hourly_sql = \"\"\"\n",
    "WITH status_hour AS (\n",
    "    SELECT date_trunc('hour', cs.time) AS hour,\n",
    "           cs.charger_id,\n",
    "           SUM(CASE WHEN cs.status IN :faults THEN 1 ELSE 0 END) AS fault_samples,\n",
    "           COUNT(*) AS samples\n",
    "    FROM charger_status cs\n",
    "    WHERE cs.time >= now() - interval '14 days'\n",
    "    GROUP BY 1,2\n",
    "), sessions_hour AS (\n",
    "    SELECT date_trunc('hour', start_time) AS hour,\n",
    "           charger_id,\n",
    "           COUNT(*) AS sessions,\n",
    "           SUM(CASE WHEN success = false THEN 1 ELSE 0 END) AS failed_sessions\n",
    "    FROM charging_sessions\n",
    "    WHERE start_time >= now() - interval '14 days'\n",
    "    GROUP BY 1,2\n",
    ")\n",
    "SELECT sh.hour, sh.charger_id,\n",
    "       COALESCE(s.sessions, 0) AS sessions,\n",
    "       COALESCE(s.failed_sessions, 0) AS failed_sessions,\n",
    "       sh.fault_samples,\n",
    "       sh.samples\n",
    "FROM status_hour sh\n",
    "LEFT JOIN sessions_hour s ON sh.hour = s.hour AND sh.charger_id = s.charger_id\n",
    "\"\"\"\n",
    "load_df = fetch_df(hourly_sql, {\"faults\": FAULT_STATES})\n",
    "load_df[\"fault_flag\"] = (load_df[\"fault_samples\"] > 0).astype(int)\n",
    "load_df[\"log_sessions\"] = np.log1p(load_df[\"sessions\"])\n",
    "load_df.head()\n",
    "\n",
    "# Logistic regression: fault_flag ~ log_sessions\n",
    "X = sm.add_constant(load_df[[\"log_sessions\"]])\n",
    "model = sm.Logit(load_df[\"fault_flag\"], X).fit(disp=False)\n",
    "print(model.summary())\n",
    "print(\"Odds ratio for sessions:\", np.exp(model.params[\"log_sessions\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b263b",
   "metadata": {},
   "source": [
    "\n",
    "## Hotspot detection (control chart style)\n",
    "Identify chargers whose fault rate over 14 days is statistically high (z-score vs portfolio average).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387dc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hot_sql = \"\"\"\n",
    "SELECT cs.charger_id,\n",
    "       c.external_id,\n",
    "       s.name AS site_name,\n",
    "       SUM(CASE WHEN cs.status IN :faults THEN 1 ELSE 0 END)::float / COUNT(*) AS fault_rate,\n",
    "       COUNT(*) AS samples\n",
    "FROM charger_status cs\n",
    "JOIN chargers c ON cs.charger_id = c.charger_id\n",
    "JOIN sites s ON c.site_id = s.site_id\n",
    "WHERE cs.time >= now() - interval '14 days'\n",
    "GROUP BY cs.charger_id, c.external_id, s.name\n",
    "HAVING COUNT(*) > 100\n",
    "\"\"\"\n",
    "hot = fetch_df(hot_sql, {\"faults\": FAULT_STATES})\n",
    "global_mean = hot.fault_rate.mean()\n",
    "global_std = hot.fault_rate.std(ddof=1)\n",
    "hot[\"z\"] = (hot.fault_rate - global_mean) / (global_std if global_std else 1)\n",
    "hotspots = hot[hot[\"z\"] > 2].sort_values(\"z\", ascending=False)\n",
    "hotspots.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e795f26",
   "metadata": {},
   "source": [
    "\n",
    "## MTBF / MTTR (survival-style)\n",
    "Estimate mean time between failures (MTBF) and mean time to repair (MTTR) using status transitions for a subset of chargers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "status_sql = \"\"\"\n",
    "SELECT time, charger_id, status\n",
    "FROM charger_status\n",
    "WHERE time >= now() - interval '30 days'\n",
    "ORDER BY charger_id, time\n",
    "\"\"\"\n",
    "status_df = fetch_df(status_sql)\n",
    "\n",
    "results = []\n",
    "for cid, group in status_df.groupby(\"charger_id\"):\n",
    "    g = group.copy()\n",
    "    g[\"is_fault\"] = g.status.isin(FAULT_STATES)\n",
    "    g[\"prev_is_fault\"] = g[\"is_fault\"].shift(fill_value=False)\n",
    "    fault_starts = g[(~g[\"prev_is_fault\"]) & (g[\"is_fault\"])]\n",
    "    fault_ends = g[(g[\"prev_is_fault\"]) & (~g[\"is_fault\"])]\n",
    "    mtbf_hours = None\n",
    "    if not fault_starts.empty:\n",
    "        mtbf_deltas = fault_starts[\"time\"].diff().dropna().dt.total_seconds() / 3600\n",
    "        if not mtbf_deltas.empty:\n",
    "            mtbf_hours = mtbf_deltas.mean()\n",
    "    mttr_hours = None\n",
    "    if not fault_ends.empty:\n",
    "        durations = (fault_ends[\"time\"] - fault_starts[\"time\"].reindex(fault_ends.index)).dt.total_seconds() / 3600\n",
    "        durations = durations.dropna()\n",
    "        if not durations.empty:\n",
    "            mttr_hours = durations.mean()\n",
    "    if mtbf_hours is not None or mttr_hours is not None:\n",
    "        results.append({\"charger_id\": cid, \"mtbf_hours\": mtbf_hours, \"mttr_hours\": mttr_hours})\n",
    "mtbf_mttr = pd.DataFrame(results).dropna(how=\"all\", subset=[\"mtbf_hours\", \"mttr_hours\"])\n",
    "mtbf_mttr.describe()[[\"mtbf_hours\", \"mttr_hours\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af0f14",
   "metadata": {},
   "source": [
    "\n",
    "## Capacity planning\n",
    "Estimate how adding capacity reduces load per charger and faults. Simple heuristic: scale fault odds by the regression coefficient from load vs faults.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "site_load_sql = \"\"\"\n",
    "SELECT s.site_id,\n",
    "       s.name AS site_name,\n",
    "       SUM(CASE WHEN cs.status IN :faults THEN 1 ELSE 0 END) AS fault_samples,\n",
    "       COUNT(*) AS status_samples,\n",
    "       COUNT(DISTINCT cs.charger_id) AS chargers,\n",
    "       COALESCE(SUM(sess.sessions_per_day), 0) AS sessions_per_day\n",
    "FROM charger_status cs\n",
    "JOIN chargers c ON cs.charger_id = c.charger_id\n",
    "JOIN sites s ON c.site_id = s.site_id\n",
    "LEFT JOIN (\n",
    "    SELECT site_id, date_trunc('day', start_time) AS day, COUNT(*) AS sessions_per_day\n",
    "    FROM charging_sessions\n",
    "    WHERE start_time >= now() - interval '14 days'\n",
    "    GROUP BY 1,2\n",
    ") sess ON sess.site_id = s.site_id\n",
    "WHERE cs.time >= now() - interval '14 days'\n",
    "GROUP BY s.site_id, s.name\n",
    "\"\"\"\n",
    "site_load = fetch_df(site_load_sql, {\"faults\": FAULT_STATES})\n",
    "site_load[\"fault_rate\"] = site_load[\"fault_samples\"] / site_load[\"status_samples\"]\n",
    "coef = model.params.get(\"log_sessions\", 0) if 'model' in globals() else 0\n",
    "site_load[\"sessions_per_charger\"] = site_load[\"sessions_per_day\"] / site_load[\"chargers\"].clip(lower=1)\n",
    "site_load[\"sessions_per_charger_plus1\"] = site_load[\"sessions_per_day\"] / (site_load[\"chargers\"] + 1)\n",
    "site_load[\"delta_log_sessions\"] = np.log1p(site_load[\"sessions_per_charger_plus1\"]) - np.log1p(site_load[\"sessions_per_charger\"])\n",
    "site_load[\"expected_fault_odds_change_pct\"] = np.expm1(coef * site_load[\"delta_log_sessions\"]) * 100\n",
    "site_load.sort_values(\"expected_fault_odds_change_pct\").head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666cc84b",
   "metadata": {},
   "source": [
    "\n",
    "## Financial impact (lost minutes) with bootstrap CI\n",
    "Estimate lost revenue using failed session minutes. Adjust `revenue_per_kwh` to match business assumptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "impact_sql = \"\"\"\n",
    "SELECT\n",
    "    COUNT(*) FILTER (WHERE success = false) AS failed_sessions,\n",
    "    SUM(duration_minutes) FILTER (WHERE success = false) AS lost_minutes,\n",
    "    SUM(energy_kwh) FILTER (WHERE success = false) AS lost_kwh\n",
    "FROM charging_sessions\n",
    "WHERE start_time >= now() - interval '30 days'\n",
    "\"\"\"\n",
    "impact = fetch_df(impact_sql).iloc[0]\n",
    "revenue_per_kwh = 0.35\n",
    "samples = 5000\n",
    "lost_kwh = impact.lost_kwh or 0\n",
    "boot = np.random.gamma(shape=lost_kwh + 1, scale=revenue_per_kwh, size=samples)\n",
    "ci = np.percentile(boot, [2.5, 50, 97.5])\n",
    "{\n",
    "    \"failed_sessions\": int(impact.failed_sessions or 0),\n",
    "    \"lost_minutes\": float(impact.lost_minutes or 0),\n",
    "    \"lost_revenue_ci\": ci,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52bad1",
   "metadata": {},
   "source": [
    "\n",
    "## Seasonality / forecasting\n",
    "Get daily fault counts and fit a quick seasonal ARIMA. Set `run_fit=True` to execute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c0146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "daily_sql = \"\"\"\n",
    "SELECT date_trunc('day', time) AS day,\n",
    "       SUM(CASE WHEN status IN :faults THEN 1 ELSE 0 END) AS faults,\n",
    "       COUNT(*) AS samples\n",
    "FROM charger_status\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\"\n",
    "daily = fetch_df(daily_sql, {\"faults\": FAULT_STATES}).set_index(\"day\")\n",
    "run_fit = False\n",
    "if run_fit and len(daily) > 7:\n",
    "    model = SARIMAX(daily[\"faults\"], order=(1,0,1), seasonal_order=(1,0,1,7)).fit(disp=False)\n",
    "    forecast = model.forecast(14)\n",
    "    print(forecast.head())\n",
    "else:\n",
    "    daily.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4161e",
   "metadata": {},
   "source": [
    "\n",
    "## Root cause hints (logistic regression)\n",
    "Model fault probability using charger metadata and time-of-day/temperature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_sql = \"\"\"\n",
    "SELECT cs.time,\n",
    "       cs.charger_id,\n",
    "       cs.status,\n",
    "       cs.temperature_celsius,\n",
    "       c.model,\n",
    "       c.connector_type\n",
    "FROM charger_status cs\n",
    "JOIN chargers c ON cs.charger_id = c.charger_id\n",
    "WHERE cs.time >= now() - interval '7 days'\n",
    "LIMIT 50000\n",
    "\"\"\"\n",
    "sample = fetch_df(sample_sql)\n",
    "sample[\"fault_flag\"] = sample.status.isin(FAULT_STATES).astype(int)\n",
    "sample[\"hour\"] = sample.time.dt.hour\n",
    "sample[\"temp\"] = sample.temperature_celsius.fillna(sample.temperature_celsius.median())\n",
    "cat = pd.get_dummies(sample[[\"model\", \"connector_type\", \"hour\"]].astype(str), drop_first=True)\n",
    "X = sm.add_constant(pd.concat([cat, sample[[\"temp\"]]], axis=1))\n",
    "logit = sm.Logit(sample[\"fault_flag\"], X).fit(disp=False)\n",
    "logit.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d33d4",
   "metadata": {},
   "source": [
    "\n",
    "## Alert tuning (precision/recall on fault-rate thresholds)\n",
    "Treat hours with any fault as positives and test a threshold on recent fault rate per charger-hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9748fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alert_sql = \"\"\"\n",
    "WITH status_hour AS (\n",
    "    SELECT date_trunc('hour', cs.time) AS hour,\n",
    "           cs.charger_id,\n",
    "           SUM(CASE WHEN cs.status IN :faults THEN 1 ELSE 0 END)::float / COUNT(*) AS fault_rate,\n",
    "           SUM(CASE WHEN cs.status IN :faults THEN 1 ELSE 0 END) AS faults\n",
    "    FROM charger_status cs\n",
    "    WHERE cs.time >= now() - interval '7 days'\n",
    "    GROUP BY 1,2\n",
    ")\n",
    "SELECT * FROM status_hour\n",
    "\"\"\"\n",
    "alert_df = fetch_df(alert_sql, {\"faults\": FAULT_STATES})\n",
    "threshold = 0.05\n",
    "alert_df[\"pred\"] = (alert_df[\"fault_rate\"] >= threshold).astype(int)\n",
    "alert_df[\"actual\"] = (alert_df[\"faults\"] > 0).astype(int)\n",
    "precision = (alert_df.query(\"pred == 1 and actual == 1\").shape[0] / max(alert_df.query(\"pred == 1\").shape[0], 1))\n",
    "recall = (alert_df.query(\"actual == 1 and pred == 1\").shape[0] / max(alert_df.query(\"actual == 1\").shape[0], 1))\n",
    "{\n",
    "    \"threshold\": threshold,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca7922",
   "metadata": {},
   "source": [
    "\n",
    "## Rollout validation (difference-in-differences)\n",
    "Template: pick treated sites and an intervention date; compare fault rates vs control sites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3acaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "treated_sites = [1, 2, 3]  # replace with real site_ids that received a change\n",
    "intervention = \"2024-01-15\"\n",
    "\n",
    "did_sql = \"\"\"\n",
    "WITH daily AS (\n",
    "    SELECT date_trunc('day', cs.time) AS day,\n",
    "           c.site_id,\n",
    "           SUM(CASE WHEN cs.status IN :faults THEN 1 ELSE 0 END)::float / COUNT(*) AS fault_rate\n",
    "    FROM charger_status cs\n",
    "    JOIN chargers c ON cs.charger_id = c.charger_id\n",
    "    GROUP BY 1, c.site_id\n",
    ")\n",
    "SELECT day,\n",
    "       site_id,\n",
    "       fault_rate,\n",
    "       CASE WHEN site_id = ANY(:treated) THEN 1 ELSE 0 END AS treated,\n",
    "       CASE WHEN day >= :intervention::date THEN 1 ELSE 0 END AS post\n",
    "FROM daily\n",
    "\"\"\"\n",
    "did = fetch_df(did_sql, {\"faults\": FAULT_STATES, \"treated\": treated_sites, \"intervention\": intervention})\n",
    "did[\"treated_post\"] = did[\"treated\"] * did[\"post\"]\n",
    "X = sm.add_constant(did[[\"treated\", \"post\", \"treated_post\"]])\n",
    "y = did[\"fault_rate\"]\n",
    "did_model = sm.OLS(y, X).fit()\n",
    "did_model.summary()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
